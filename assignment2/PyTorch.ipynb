{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ConvNet PyTorch\n",
    "\n",
    "In this notebook, you'll learn how to use the powerful PyTorch framework to specify a conv net architecture and train it on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this PyTorch business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, PyTorch (or TensorFlow, if you switch over to that notebook). \n",
    "\n",
    "Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. When using a framework like PyTorch or TensorFlow you can harness the power of the GPU for your own custom neural network architectures without having to write CUDA code directly (which is beyond the scope of this class).\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn PyTorch?\n",
    "\n",
    "If you've used Torch before, but are new to PyTorch, this tutorial might be of use: http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in Torch. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "We load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "NUM_VAL = 1000\n",
    "\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,\n",
    "                          transform=T.ToTensor())\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we're going to use a CPU-friendly datatype. Later, we'll switch to a datatype that will move all our computations to the GPU and measure the speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "### Some assorted tidbits\n",
    "\n",
    "Let's start by looking at a simple model. First, note that PyTorch operates on Tensors, which are n-dimensional arrays functionally analogous to numpy's ndarrays, with the additional feature that they can be used for computations on GPUs.\n",
    "\n",
    "We'll provide you with a Flatten function, which we explain here. Remember that our image data (and more relevantly, our intermediate feature maps) are initially N x C x H x W, where:\n",
    "* N is the number of datapoints\n",
    "* C is the number of channels\n",
    "* H is the height of the intermediate feature map in pixels\n",
    "* W is the width of the intermediate feature map in pixels\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we input  data into fully connected affine layers, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"Flatten\" operation to collapse the C x H x W values per representation into a single long vector. The Flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in PyTorch -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up.  nn.Sequential is a container which applies each layer\n",
    "one after the other.\n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Cross-Entropy loss function, and the Adam optimizer being used. \n",
    "\n",
    "Make sure you understand why the parameters of the Linear layer are 5408 and 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's where we define the architecture of the model... \n",
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(5408, 10), # affine layer\n",
    "              )\n",
    "\n",
    "# Set the type of all data in this model to be FloatTensor \n",
    "simple_model.type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=1e-2) # lr sets the learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). One note: what we call in the class \"spatial batch norm\" is called \"BatchNorm2D\" in PyTorch.\n",
    "\n",
    "* Layers: http://pytorch.org/docs/nn.html\n",
    "* Activations: http://pytorch.org/docs/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a specific model\n",
    "\n",
    "In this section, we're going to specify a model for you to construct. The goal here isn't to get good performance (that'll be next), but instead to get comfortable with understanding the PyTorch documentation and configuring your own model. \n",
    "\n",
    "Using the code provided above as guidance, and using the following PyTorch documentation, specify a model with the following architecture:\n",
    "\n",
    "* 7x7 Convolutional Layer with 32 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* Spatial Batch Normalization Layer\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* Affine layer with 1024 output units\n",
    "* ReLU Activation Layer\n",
    "* Affine layer from 1024 input units to 10 outputs\n",
    "\n",
    "And finally, set up a **cross-entropy** loss function and the **RMSprop** learning rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_model_base = nn.Sequential( # You fill this in!\n",
    "    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(num_features=32),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    nn.Linear(in_features=5408, out_features=1024),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=1024, out_features=10)\n",
    ")\n",
    "\n",
    "fixed_model = fixed_model_base.type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're doing the right thing, use the following tool to check the dimensionality of your output (it should be 64 x 10, since our batches have size 64 and the output of the final affine layer should be 10, corresponding to our 10 classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we're going to feed a random batch into the model you defined and make sure the output is the right size\n",
    "x = torch.randn(64, 3, 32, 32).type(dtype)\n",
    "x_var = Variable(x.type(dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model(x_var)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10]))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU!\n",
    "\n",
    "Now, we're going to switch the dtype of the model and our data to the GPU-friendly tensors, and see what happens... everything is the same, except we are casting our model and input tensors as this new dtype instead of the old one.\n",
    "\n",
    "If this returns false, or otherwise fails in a not-graceful way (i.e., with some error message), you may not have an NVIDIA GPU available on your machine. If you're running locally, we recommend you switch to Google Cloud and follow the instructions to set up a GPU there. If you're already on Google Cloud, something is wrong -- make sure you followed the instructions on how to request and use a GPU on your instance. If you did, post on Piazza or come to Office Hours so we can help you debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that CUDA is properly configured and you have a GPU available\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "#gpu_dtype = torch.FloatTensor\n",
    "\n",
    "fixed_model_gpu = copy.deepcopy(fixed_model_base).type(gpu_dtype)\n",
    "\n",
    "x_gpu = torch.randn(64, 3, 32, 32).type(gpu_dtype)\n",
    "x_var_gpu = Variable(x.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to evaluate the performance of the forward pass running on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.3 ms ± 2.79 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "ans = fixed_model(x_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426 µs ± 24.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that even a simple forward pass like this is significantly faster on the GPU. So for the rest of the assignment (and when you go train your models in assignment 3 and your project!), you should use the GPU datatype for your model and your tensors: as a reminder that is *torch.cuda.FloatTensor* (in our notebook here as *gpu_dtype*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model.\n",
    "\n",
    "Now that you've seen how to define a model and do a single forward pass of some data through it, let's  walk through how you'd actually train one whole epoch over your training data (using the simple_model we provided above).\n",
    "\n",
    "Make sure you understand how each PyTorch function used below corresponds to what you implemented in your custom neural network implementation.\n",
    "\n",
    "Note that because we are not resetting the weights anywhere below, if you run the cell multiple times, you are effectively training multiple epochs (so your performance should improve).\n",
    "\n",
    "First, set up an RMSprop optimizer (using a 1e-3 learning rate) and a cross-entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.RMSprop(fixed_model_gpu.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 1.4150\n",
      "t = 200, loss = 1.5418\n",
      "t = 300, loss = 1.4233\n",
      "t = 400, loss = 1.3047\n",
      "t = 500, loss = 1.2100\n",
      "t = 600, loss = 1.5451\n",
      "t = 700, loss = 1.2757\n"
     ]
    }
   ],
   "source": [
    "# This sets the model in \"training\" mode. This is relevant for some layers that may have different behavior\n",
    "# in training mode vs testing mode, such as Dropout and BatchNorm. \n",
    "fixed_model_gpu.train()\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = fixed_model_gpu(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen how the training process works in PyTorch. To save you writing boilerplate code, we're providing the following helper functions to help you train for multiple epochs and check the accuracy of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, loss_fn, optimizer, lr_scheduler=None, num_epochs=1, trace_acc=False):\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    t0 = time.time()\n",
    "    times = {\"epochs\": []}\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        t1 = time.time()\n",
    "        \n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "            loss_history.append(loss.data[0])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            if isinstance(lr_scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                lr_scheduler.step(loss_history[-1])\n",
    "            else:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "        training_time = time.time() - t1\n",
    "        print(\"Epoch training time: %.3f\" % training_time)\n",
    "        \n",
    "        acc_time = None\n",
    "        if trace_acc or epoch + 1 == num_epochs:\n",
    "            t2 = time.time()\n",
    "            train_acc_history.append(check_accuracy(model, loader_train))\n",
    "            val_acc_history.append(check_accuracy(model, loader_val))\n",
    "            acc_time = time.time() - t2\n",
    "            print(\"Epoch accuracy time: %.3f\" % acc_time)\n",
    "        times[\"epochs\"].append({\"train\": training_time, \"acc\": acc_time})\n",
    "    \n",
    "    total_time = time.time() - t0\n",
    "    print(\"Total time: %.3f\" % total_time)\n",
    "    times[\"total\"] = total_time\n",
    "    return {\n",
    "        \"loss\": loss_history,\n",
    "        \"train_acc\": train_acc_history,\n",
    "        \"val_acc\": val_acc_history,\n",
    "        \"times\": times\n",
    "    }\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    if not loader.dataset.train:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the model.\n",
    "\n",
    "Let's see the train and check_accuracy code in action -- feel free to use these methods when evaluating the models you develop below.\n",
    "\n",
    "You should get a training loss of around 1.2-1.4, and a validation accuracy of around 50-60%. As mentioned above, if you re-run the cells, you'll be training more epochs, so your performance will improve past these numbers.\n",
    "\n",
    "But don't worry about getting these numbers better -- this was just practice before you tackle designing your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1\n",
      "t = 100, loss = 1.4056\n",
      "t = 200, loss = 1.5512\n",
      "t = 300, loss = 1.3797\n",
      "t = 400, loss = 1.1603\n",
      "t = 500, loss = 1.1897\n",
      "t = 600, loss = 1.2848\n",
      "t = 700, loss = 1.3046\n",
      "Epoch training time: 5.370\n",
      "Got 28626 / 49000 correct (58.42)\n",
      "Got 534 / 1000 correct (53.40)\n",
      "Epoch accuracy time: 4.605\n",
      "Total time: 9.976\n",
      "Got 534 / 1000 correct (53.40)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.random.manual_seed(12345)\n",
    "torch.random.manual_seed(12345)\n",
    "fixed_model_gpu.apply(reset)\n",
    "train(fixed_model_gpu, loss_fn, optimizer, num_epochs=1)\n",
    "acc = check_accuracy(fixed_model_gpu, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget the validation set!\n",
    "\n",
    "And note that you can use the check_accuracy function to evaluate on either the test set or the validation set, by passing either **loader_test** or **loader_val** as the second argument to check_accuracy. You should not touch the test set until you have finished your architecture and hyperparameter tuning, and only run the test set once at the end to report a final value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a _great_ model on CIFAR-10!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves **>=70%** accuracy on the CIFAR-10 **validation** set. You can use the check_accuracy and train functions from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use Dropout.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 70% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. \n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Files already downloaded and verified\n",
      "Params: {'learning_rate': 0.0002, 'l2': 0.0006, 'dropout': 0.22, 'num_epochs': 240, 'optimizer_class': <class 'torch.optim.adam.Adam'>, 'model_builder': <function conv_conv_pool_2_dr_bn at 0x7fd598f58ae8>, 'loss_fn': CrossEntropyLoss(\n",
      "), 'transforms_fn': <function transforms_6 at 0x7fd598f399d8>, 'lr_scheduler': <function lr_scheduler_step_4 at 0x7fd598f397b8>, 'batch_size': 256}\n",
      "Epoch 1 / 240\n",
      "t = 100, loss = 1.9809\n",
      "Epoch training time: 13.033\n",
      "Got 16841 / 49000 correct (34.37)\n",
      "Got 407 / 1000 correct (40.70)\n",
      "Epoch accuracy time: 7.333\n",
      "Epoch 2 / 240\n",
      "t = 100, loss = 1.6128\n",
      "Epoch training time: 13.100\n",
      "Got 21614 / 49000 correct (44.11)\n",
      "Got 508 / 1000 correct (50.80)\n",
      "Epoch accuracy time: 7.131\n",
      "Epoch 3 / 240\n",
      "t = 100, loss = 1.5412\n",
      "Epoch training time: 13.189\n",
      "Got 24607 / 49000 correct (50.22)\n",
      "Got 556 / 1000 correct (55.60)\n",
      "Epoch accuracy time: 7.158\n",
      "Epoch 4 / 240\n",
      "t = 100, loss = 1.3190\n",
      "Epoch training time: 13.266\n",
      "Got 27597 / 49000 correct (56.32)\n",
      "Got 613 / 1000 correct (61.30)\n",
      "Epoch accuracy time: 7.199\n",
      "Epoch 5 / 240\n",
      "t = 100, loss = 1.3092\n",
      "Epoch training time: 13.302\n",
      "Got 29266 / 49000 correct (59.73)\n",
      "Got 638 / 1000 correct (63.80)\n",
      "Epoch accuracy time: 7.123\n",
      "Epoch 6 / 240\n",
      "t = 100, loss = 1.1756\n",
      "Epoch training time: 13.320\n",
      "Got 31053 / 49000 correct (63.37)\n",
      "Got 676 / 1000 correct (67.60)\n",
      "Epoch accuracy time: 7.126\n",
      "Epoch 7 / 240\n",
      "t = 100, loss = 1.1847\n",
      "Epoch training time: 13.327\n",
      "Got 32774 / 49000 correct (66.89)\n",
      "Got 689 / 1000 correct (68.90)\n",
      "Epoch accuracy time: 7.118\n",
      "Epoch 8 / 240\n",
      "t = 100, loss = 1.0781\n",
      "Epoch training time: 13.356\n",
      "Got 33708 / 49000 correct (68.79)\n",
      "Got 718 / 1000 correct (71.80)\n",
      "Epoch accuracy time: 7.207\n",
      "Epoch 9 / 240\n",
      "t = 100, loss = 1.0226\n",
      "Epoch training time: 13.376\n",
      "Got 34692 / 49000 correct (70.80)\n",
      "Got 744 / 1000 correct (74.40)\n",
      "Epoch accuracy time: 7.173\n",
      "Epoch 10 / 240\n",
      "t = 100, loss = 0.9557\n",
      "Epoch training time: 13.375\n",
      "Got 35709 / 49000 correct (72.88)\n",
      "Got 755 / 1000 correct (75.50)\n",
      "Epoch accuracy time: 7.147\n",
      "Epoch 11 / 240\n",
      "t = 100, loss = 0.9144\n",
      "Epoch training time: 13.379\n",
      "Got 35993 / 49000 correct (73.46)\n",
      "Got 758 / 1000 correct (75.80)\n",
      "Epoch accuracy time: 7.167\n",
      "Epoch 12 / 240\n",
      "t = 100, loss = 0.8862\n",
      "Epoch training time: 13.344\n",
      "Got 36612 / 49000 correct (74.72)\n",
      "Got 776 / 1000 correct (77.60)\n",
      "Epoch accuracy time: 7.219\n",
      "Epoch 13 / 240\n",
      "t = 100, loss = 0.8447\n",
      "Epoch training time: 13.387\n",
      "Got 37172 / 49000 correct (75.86)\n",
      "Got 794 / 1000 correct (79.40)\n",
      "Epoch accuracy time: 7.163\n",
      "Epoch 14 / 240\n",
      "t = 100, loss = 0.8849\n",
      "Epoch training time: 13.381\n",
      "Got 37433 / 49000 correct (76.39)\n",
      "Got 802 / 1000 correct (80.20)\n",
      "Epoch accuracy time: 7.222\n",
      "Epoch 15 / 240\n",
      "t = 100, loss = 0.8314\n",
      "Epoch training time: 13.411\n",
      "Got 37772 / 49000 correct (77.09)\n",
      "Got 802 / 1000 correct (80.20)\n",
      "Epoch accuracy time: 7.203\n",
      "Epoch 16 / 240\n",
      "t = 100, loss = 0.8258\n",
      "Epoch training time: 13.423\n",
      "Got 38269 / 49000 correct (78.10)\n",
      "Got 813 / 1000 correct (81.30)\n",
      "Epoch accuracy time: 7.169\n",
      "Epoch 17 / 240\n",
      "t = 100, loss = 0.7615\n",
      "Epoch training time: 13.385\n",
      "Got 38435 / 49000 correct (78.44)\n",
      "Got 815 / 1000 correct (81.50)\n",
      "Epoch accuracy time: 7.151\n",
      "Epoch 18 / 240\n",
      "t = 100, loss = 0.7978\n",
      "Epoch training time: 13.387\n",
      "Got 38668 / 49000 correct (78.91)\n",
      "Got 811 / 1000 correct (81.10)\n",
      "Epoch accuracy time: 7.181\n",
      "Epoch 19 / 240\n",
      "t = 100, loss = 0.7566\n",
      "Epoch training time: 13.457\n",
      "Got 39181 / 49000 correct (79.96)\n",
      "Got 812 / 1000 correct (81.20)\n",
      "Epoch accuracy time: 7.225\n",
      "Epoch 20 / 240\n",
      "t = 100, loss = 0.7798\n",
      "Epoch training time: 13.381\n",
      "Got 39175 / 49000 correct (79.95)\n",
      "Got 816 / 1000 correct (81.60)\n",
      "Epoch accuracy time: 7.095\n",
      "Epoch 21 / 240\n",
      "t = 100, loss = 0.7966\n",
      "Epoch training time: 13.390\n",
      "Got 39295 / 49000 correct (80.19)\n",
      "Got 822 / 1000 correct (82.20)\n",
      "Epoch accuracy time: 7.189\n",
      "Epoch 22 / 240\n",
      "t = 100, loss = 0.7145\n",
      "Epoch training time: 13.386\n",
      "Got 39687 / 49000 correct (80.99)\n",
      "Got 822 / 1000 correct (82.20)\n",
      "Epoch accuracy time: 7.121\n",
      "Epoch 23 / 240\n",
      "t = 100, loss = 0.7209\n",
      "Epoch training time: 13.397\n",
      "Got 40320 / 49000 correct (82.29)\n",
      "Got 835 / 1000 correct (83.50)\n",
      "Epoch accuracy time: 7.166\n",
      "Epoch 24 / 240\n",
      "t = 100, loss = 0.6710\n",
      "Epoch training time: 13.386\n",
      "Got 40260 / 49000 correct (82.16)\n",
      "Got 838 / 1000 correct (83.80)\n",
      "Epoch accuracy time: 7.138\n",
      "Epoch 25 / 240\n",
      "t = 100, loss = 0.6442\n",
      "Epoch training time: 13.387\n",
      "Got 40322 / 49000 correct (82.29)\n",
      "Got 845 / 1000 correct (84.50)\n",
      "Epoch accuracy time: 7.120\n",
      "Epoch 26 / 240\n",
      "t = 100, loss = 0.6646\n",
      "Epoch training time: 13.429\n",
      "Got 40590 / 49000 correct (82.84)\n",
      "Got 830 / 1000 correct (83.00)\n",
      "Epoch accuracy time: 7.123\n",
      "Epoch 27 / 240\n",
      "t = 100, loss = 0.6543\n",
      "Epoch training time: 13.391\n",
      "Got 40493 / 49000 correct (82.64)\n",
      "Got 833 / 1000 correct (83.30)\n",
      "Epoch accuracy time: 7.202\n",
      "Epoch 28 / 240\n",
      "t = 100, loss = 0.6839\n",
      "Epoch training time: 13.446\n",
      "Got 41045 / 49000 correct (83.77)\n",
      "Got 848 / 1000 correct (84.80)\n",
      "Epoch accuracy time: 7.193\n",
      "Epoch 29 / 240\n",
      "t = 100, loss = 0.7142\n",
      "Epoch training time: 13.405\n",
      "Got 41031 / 49000 correct (83.74)\n",
      "Got 849 / 1000 correct (84.90)\n",
      "Epoch accuracy time: 7.093\n",
      "Epoch 30 / 240\n",
      "t = 100, loss = 0.5986\n",
      "Epoch training time: 13.441\n",
      "Got 41116 / 49000 correct (83.91)\n",
      "Got 848 / 1000 correct (84.80)\n",
      "Epoch accuracy time: 7.474\n",
      "Epoch 31 / 240\n",
      "t = 100, loss = 0.6755\n",
      "Epoch training time: 13.439\n",
      "Got 41272 / 49000 correct (84.23)\n",
      "Got 842 / 1000 correct (84.20)\n",
      "Epoch accuracy time: 7.217\n",
      "Epoch 32 / 240\n",
      "t = 100, loss = 0.6168\n",
      "Epoch training time: 13.406\n",
      "Got 41380 / 49000 correct (84.45)\n",
      "Got 842 / 1000 correct (84.20)\n",
      "Epoch accuracy time: 7.140\n",
      "Epoch 33 / 240\n",
      "t = 100, loss = 0.6044\n",
      "Epoch training time: 13.396\n",
      "Got 41586 / 49000 correct (84.87)\n",
      "Got 851 / 1000 correct (85.10)\n",
      "Epoch accuracy time: 7.135\n",
      "Epoch 34 / 240\n",
      "t = 100, loss = 0.6361\n",
      "Epoch training time: 13.385\n",
      "Got 41464 / 49000 correct (84.62)\n",
      "Got 853 / 1000 correct (85.30)\n",
      "Epoch accuracy time: 7.199\n",
      "Epoch 35 / 240\n",
      "t = 100, loss = 0.5144\n",
      "Epoch training time: 13.419\n",
      "Got 41681 / 49000 correct (85.06)\n",
      "Got 852 / 1000 correct (85.20)\n",
      "Epoch accuracy time: 7.154\n",
      "Epoch 36 / 240\n",
      "t = 100, loss = 0.6117\n",
      "Epoch training time: 13.404\n",
      "Got 42084 / 49000 correct (85.89)\n",
      "Got 846 / 1000 correct (84.60)\n",
      "Epoch accuracy time: 7.188\n",
      "Epoch 37 / 240\n",
      "t = 100, loss = 0.6369\n",
      "Epoch training time: 13.445\n",
      "Got 41938 / 49000 correct (85.59)\n",
      "Got 850 / 1000 correct (85.00)\n",
      "Epoch accuracy time: 7.138\n",
      "Epoch 38 / 240\n",
      "t = 100, loss = 0.6151\n",
      "Epoch training time: 13.386\n",
      "Got 42164 / 49000 correct (86.05)\n",
      "Got 857 / 1000 correct (85.70)\n",
      "Epoch accuracy time: 7.110\n",
      "Epoch 39 / 240\n",
      "t = 100, loss = 0.5971\n",
      "Epoch training time: 13.437\n",
      "Got 42318 / 49000 correct (86.36)\n",
      "Got 864 / 1000 correct (86.40)\n",
      "Epoch accuracy time: 7.140\n",
      "Epoch 40 / 240\n",
      "t = 100, loss = 0.5775\n",
      "Epoch training time: 13.415\n",
      "Got 42202 / 49000 correct (86.13)\n",
      "Got 849 / 1000 correct (84.90)\n",
      "Epoch accuracy time: 7.130\n",
      "Epoch 41 / 240\n",
      "t = 100, loss = 0.5752\n",
      "Epoch training time: 13.409\n",
      "Got 42595 / 49000 correct (86.93)\n",
      "Got 862 / 1000 correct (86.20)\n",
      "Epoch accuracy time: 7.161\n",
      "Epoch 42 / 240\n",
      "t = 100, loss = 0.5829\n",
      "Epoch training time: 13.388\n",
      "Got 42701 / 49000 correct (87.14)\n",
      "Got 858 / 1000 correct (85.80)\n",
      "Epoch accuracy time: 7.088\n",
      "Epoch 43 / 240\n",
      "t = 100, loss = 0.5294\n",
      "Epoch training time: 13.412\n",
      "Got 42653 / 49000 correct (87.05)\n",
      "Got 864 / 1000 correct (86.40)\n",
      "Epoch accuracy time: 7.146\n",
      "Epoch 44 / 240\n",
      "t = 100, loss = 0.6108\n",
      "Epoch training time: 13.444\n",
      "Got 43043 / 49000 correct (87.84)\n",
      "Got 863 / 1000 correct (86.30)\n",
      "Epoch accuracy time: 7.103\n",
      "Epoch 45 / 240\n",
      "t = 100, loss = 0.4962\n",
      "Epoch training time: 13.398\n",
      "Got 42979 / 49000 correct (87.71)\n",
      "Got 860 / 1000 correct (86.00)\n",
      "Epoch accuracy time: 7.193\n",
      "Epoch 46 / 240\n",
      "t = 100, loss = 0.5814\n",
      "Epoch training time: 13.405\n",
      "Got 43146 / 49000 correct (88.05)\n",
      "Got 868 / 1000 correct (86.80)\n",
      "Epoch accuracy time: 7.170\n",
      "Epoch 47 / 240\n",
      "t = 100, loss = 0.5327\n",
      "Epoch training time: 13.400\n",
      "Got 43014 / 49000 correct (87.78)\n",
      "Got 871 / 1000 correct (87.10)\n",
      "Epoch accuracy time: 7.149\n",
      "Epoch 48 / 240\n",
      "t = 100, loss = 0.5523\n",
      "Epoch training time: 13.413\n",
      "Got 43037 / 49000 correct (87.83)\n",
      "Got 875 / 1000 correct (87.50)\n",
      "Epoch accuracy time: 7.199\n",
      "Epoch 49 / 240\n",
      "t = 100, loss = 0.5213\n",
      "Epoch training time: 13.409\n",
      "Got 43378 / 49000 correct (88.53)\n",
      "Got 871 / 1000 correct (87.10)\n",
      "Epoch accuracy time: 7.140\n",
      "Epoch 50 / 240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 0.5335\n",
      "Epoch training time: 13.428\n",
      "Got 43235 / 49000 correct (88.23)\n",
      "Got 870 / 1000 correct (87.00)\n",
      "Epoch accuracy time: 7.165\n",
      "Epoch 51 / 240\n",
      "t = 100, loss = 0.5206\n",
      "Epoch training time: 13.400\n",
      "Got 43624 / 49000 correct (89.03)\n",
      "Got 876 / 1000 correct (87.60)\n",
      "Epoch accuracy time: 7.141\n",
      "Epoch 52 / 240\n",
      "t = 100, loss = 0.5496\n",
      "Epoch training time: 13.411\n",
      "Got 43544 / 49000 correct (88.87)\n",
      "Got 879 / 1000 correct (87.90)\n",
      "Epoch accuracy time: 7.196\n",
      "Epoch 53 / 240\n",
      "t = 100, loss = 0.5744\n",
      "Epoch training time: 13.395\n",
      "Got 43472 / 49000 correct (88.72)\n",
      "Got 881 / 1000 correct (88.10)\n",
      "Epoch accuracy time: 7.205\n",
      "Epoch 54 / 240\n",
      "t = 100, loss = 0.4454\n",
      "Epoch training time: 13.380\n",
      "Got 43691 / 49000 correct (89.17)\n",
      "Got 878 / 1000 correct (87.80)\n",
      "Epoch accuracy time: 7.213\n",
      "Epoch 55 / 240\n",
      "t = 100, loss = 0.5037\n",
      "Epoch training time: 13.397\n",
      "Got 43932 / 49000 correct (89.66)\n",
      "Got 885 / 1000 correct (88.50)\n",
      "Epoch accuracy time: 7.169\n",
      "Epoch 56 / 240\n",
      "t = 100, loss = 0.5095\n",
      "Epoch training time: 13.390\n",
      "Got 43974 / 49000 correct (89.74)\n",
      "Got 886 / 1000 correct (88.60)\n",
      "Epoch accuracy time: 7.088\n",
      "Epoch 57 / 240\n",
      "t = 100, loss = 0.5039\n",
      "Epoch training time: 13.396\n",
      "Got 44036 / 49000 correct (89.87)\n",
      "Got 881 / 1000 correct (88.10)\n",
      "Epoch accuracy time: 7.078\n",
      "Epoch 58 / 240\n",
      "t = 100, loss = 0.5508\n",
      "Epoch training time: 13.424\n",
      "Got 44099 / 49000 correct (90.00)\n",
      "Got 888 / 1000 correct (88.80)\n",
      "Epoch accuracy time: 7.081\n",
      "Epoch 59 / 240\n",
      "t = 100, loss = 0.4710\n",
      "Epoch training time: 13.392\n",
      "Got 44236 / 49000 correct (90.28)\n",
      "Got 888 / 1000 correct (88.80)\n",
      "Epoch accuracy time: 7.143\n",
      "Epoch 60 / 240\n",
      "t = 100, loss = 0.4630\n",
      "Epoch training time: 13.395\n",
      "Got 44278 / 49000 correct (90.36)\n",
      "Got 887 / 1000 correct (88.70)\n",
      "Epoch accuracy time: 7.144\n",
      "Epoch 61 / 240\n",
      "t = 100, loss = 0.4784\n",
      "Epoch training time: 13.379\n",
      "Got 44165 / 49000 correct (90.13)\n",
      "Got 882 / 1000 correct (88.20)\n",
      "Epoch accuracy time: 7.108\n",
      "Epoch 62 / 240\n",
      "t = 100, loss = 0.4476\n",
      "Epoch training time: 13.419\n",
      "Got 44225 / 49000 correct (90.26)\n",
      "Got 892 / 1000 correct (89.20)\n",
      "Epoch accuracy time: 7.248\n",
      "Epoch 63 / 240\n",
      "t = 100, loss = 0.4830\n",
      "Epoch training time: 13.391\n",
      "Got 44415 / 49000 correct (90.64)\n",
      "Got 894 / 1000 correct (89.40)\n",
      "Epoch accuracy time: 7.134\n",
      "Epoch 64 / 240\n",
      "t = 100, loss = 0.5114\n",
      "Epoch training time: 13.399\n",
      "Got 44538 / 49000 correct (90.89)\n",
      "Got 891 / 1000 correct (89.10)\n",
      "Epoch accuracy time: 7.135\n",
      "Epoch 65 / 240\n",
      "t = 100, loss = 0.4629\n",
      "Epoch training time: 13.406\n",
      "Got 44757 / 49000 correct (91.34)\n",
      "Got 883 / 1000 correct (88.30)\n",
      "Epoch accuracy time: 7.125\n",
      "Epoch 66 / 240\n",
      "t = 100, loss = 0.4981\n",
      "Epoch training time: 13.394\n",
      "Got 44558 / 49000 correct (90.93)\n",
      "Got 892 / 1000 correct (89.20)\n",
      "Epoch accuracy time: 7.124\n",
      "Epoch 67 / 240\n",
      "t = 100, loss = 0.4476\n",
      "Epoch training time: 13.387\n",
      "Got 44720 / 49000 correct (91.27)\n",
      "Got 881 / 1000 correct (88.10)\n",
      "Epoch accuracy time: 7.115\n",
      "Epoch 68 / 240\n",
      "t = 100, loss = 0.4981\n",
      "Epoch training time: 13.403\n",
      "Got 44606 / 49000 correct (91.03)\n",
      "Got 887 / 1000 correct (88.70)\n",
      "Epoch accuracy time: 7.135\n",
      "Epoch 69 / 240\n",
      "t = 100, loss = 0.5279\n",
      "Epoch training time: 13.391\n",
      "Got 44703 / 49000 correct (91.23)\n",
      "Got 883 / 1000 correct (88.30)\n",
      "Epoch accuracy time: 7.167\n",
      "Epoch 70 / 240\n",
      "t = 100, loss = 0.3807\n",
      "Epoch training time: 13.414\n",
      "Got 44781 / 49000 correct (91.39)\n",
      "Got 880 / 1000 correct (88.00)\n",
      "Epoch accuracy time: 7.166\n",
      "Epoch 71 / 240\n",
      "t = 100, loss = 0.4963\n",
      "Epoch training time: 13.399\n",
      "Got 44802 / 49000 correct (91.43)\n",
      "Got 882 / 1000 correct (88.20)\n",
      "Epoch accuracy time: 7.099\n",
      "Epoch 72 / 240\n",
      "t = 100, loss = 0.4330\n",
      "Epoch training time: 13.397\n",
      "Got 45169 / 49000 correct (92.18)\n",
      "Got 886 / 1000 correct (88.60)\n",
      "Epoch accuracy time: 7.157\n",
      "Epoch 73 / 240\n",
      "t = 100, loss = 0.4111\n",
      "Epoch training time: 13.382\n",
      "Got 45100 / 49000 correct (92.04)\n",
      "Got 884 / 1000 correct (88.40)\n",
      "Epoch accuracy time: 7.177\n",
      "Epoch 74 / 240\n",
      "t = 100, loss = 0.3555\n",
      "Epoch training time: 13.370\n",
      "Got 44908 / 49000 correct (91.65)\n",
      "Got 889 / 1000 correct (88.90)\n",
      "Epoch accuracy time: 7.145\n",
      "Epoch 75 / 240\n",
      "t = 100, loss = 0.3664\n",
      "Epoch training time: 13.393\n",
      "Got 45170 / 49000 correct (92.18)\n",
      "Got 888 / 1000 correct (88.80)\n",
      "Epoch accuracy time: 7.110\n",
      "Epoch 76 / 240\n",
      "t = 100, loss = 0.4226\n",
      "Epoch training time: 13.383\n",
      "Got 45250 / 49000 correct (92.35)\n",
      "Got 884 / 1000 correct (88.40)\n",
      "Epoch accuracy time: 7.099\n",
      "Epoch 77 / 240\n",
      "t = 100, loss = 0.4457\n",
      "Epoch training time: 13.373\n",
      "Got 45049 / 49000 correct (91.94)\n",
      "Got 885 / 1000 correct (88.50)\n",
      "Epoch accuracy time: 7.220\n",
      "Epoch 78 / 240\n",
      "t = 100, loss = 0.4170\n",
      "Epoch training time: 13.424\n",
      "Got 45383 / 49000 correct (92.62)\n",
      "Got 893 / 1000 correct (89.30)\n",
      "Epoch accuracy time: 7.094\n",
      "Epoch 79 / 240\n",
      "t = 100, loss = 0.4422\n",
      "Epoch training time: 13.374\n",
      "Got 45459 / 49000 correct (92.77)\n",
      "Got 892 / 1000 correct (89.20)\n",
      "Epoch accuracy time: 7.077\n",
      "Epoch 80 / 240\n",
      "t = 100, loss = 0.4169\n",
      "Epoch training time: 13.401\n",
      "Got 45060 / 49000 correct (91.96)\n",
      "Got 887 / 1000 correct (88.70)\n",
      "Epoch accuracy time: 7.183\n",
      "Epoch 81 / 240\n",
      "t = 100, loss = 0.4350\n",
      "Epoch training time: 13.369\n",
      "Got 45251 / 49000 correct (92.35)\n",
      "Got 898 / 1000 correct (89.80)\n",
      "Epoch accuracy time: 7.149\n",
      "Epoch 82 / 240\n",
      "t = 100, loss = 0.4428\n",
      "Epoch training time: 13.390\n",
      "Got 45409 / 49000 correct (92.67)\n",
      "Got 886 / 1000 correct (88.60)\n",
      "Epoch accuracy time: 7.154\n",
      "Epoch 83 / 240\n",
      "t = 100, loss = 0.3491\n",
      "Epoch training time: 13.364\n",
      "Got 45438 / 49000 correct (92.73)\n",
      "Got 898 / 1000 correct (89.80)\n",
      "Epoch accuracy time: 7.131\n",
      "Epoch 84 / 240\n",
      "t = 100, loss = 0.4273\n",
      "Epoch training time: 13.395\n",
      "Got 45555 / 49000 correct (92.97)\n",
      "Got 899 / 1000 correct (89.90)\n",
      "Epoch accuracy time: 7.168\n",
      "Epoch 85 / 240\n",
      "t = 100, loss = 0.3994\n",
      "Epoch training time: 13.382\n",
      "Got 45459 / 49000 correct (92.77)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.109\n",
      "Epoch 86 / 240\n",
      "t = 100, loss = 0.3688\n",
      "Epoch training time: 13.404\n",
      "Got 45843 / 49000 correct (93.56)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.117\n",
      "Epoch 87 / 240\n",
      "t = 100, loss = 0.3748\n",
      "Epoch training time: 13.418\n",
      "Got 45691 / 49000 correct (93.25)\n",
      "Got 904 / 1000 correct (90.40)\n",
      "Epoch accuracy time: 7.135\n",
      "Epoch 88 / 240\n",
      "t = 100, loss = 0.3642\n",
      "Epoch training time: 13.406\n",
      "Got 45427 / 49000 correct (92.71)\n",
      "Got 890 / 1000 correct (89.00)\n",
      "Epoch accuracy time: 7.203\n",
      "Epoch 89 / 240\n",
      "t = 100, loss = 0.3644\n",
      "Epoch training time: 13.386\n",
      "Got 45874 / 49000 correct (93.62)\n",
      "Got 892 / 1000 correct (89.20)\n",
      "Epoch accuracy time: 7.103\n",
      "Epoch 90 / 240\n",
      "t = 100, loss = 0.4349\n",
      "Epoch training time: 13.385\n",
      "Got 45871 / 49000 correct (93.61)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.086\n",
      "Epoch 91 / 240\n",
      "t = 100, loss = 0.4088\n",
      "Epoch training time: 13.381\n",
      "Got 45946 / 49000 correct (93.77)\n",
      "Got 903 / 1000 correct (90.30)\n",
      "Epoch accuracy time: 7.185\n",
      "Epoch 92 / 240\n",
      "t = 100, loss = 0.3835\n",
      "Epoch training time: 13.397\n",
      "Got 46051 / 49000 correct (93.98)\n",
      "Got 892 / 1000 correct (89.20)\n",
      "Epoch accuracy time: 7.069\n",
      "Epoch 93 / 240\n",
      "t = 100, loss = 0.4178\n",
      "Epoch training time: 13.416\n",
      "Got 45815 / 49000 correct (93.50)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.192\n",
      "Epoch 94 / 240\n",
      "t = 100, loss = 0.3505\n",
      "Epoch training time: 13.402\n",
      "Got 46027 / 49000 correct (93.93)\n",
      "Got 886 / 1000 correct (88.60)\n",
      "Epoch accuracy time: 7.224\n",
      "Epoch 95 / 240\n",
      "t = 100, loss = 0.3834\n",
      "Epoch training time: 13.413\n",
      "Got 45888 / 49000 correct (93.65)\n",
      "Got 898 / 1000 correct (89.80)\n",
      "Epoch accuracy time: 7.190\n",
      "Epoch 96 / 240\n",
      "t = 100, loss = 0.3224\n",
      "Epoch training time: 13.401\n",
      "Got 46251 / 49000 correct (94.39)\n",
      "Got 898 / 1000 correct (89.80)\n",
      "Epoch accuracy time: 7.125\n",
      "Epoch 97 / 240\n",
      "t = 100, loss = 0.3627\n",
      "Epoch training time: 13.380\n",
      "Got 45976 / 49000 correct (93.83)\n",
      "Got 895 / 1000 correct (89.50)\n",
      "Epoch accuracy time: 7.200\n",
      "Epoch 98 / 240\n",
      "t = 100, loss = 0.3065\n",
      "Epoch training time: 13.390\n",
      "Got 46358 / 49000 correct (94.61)\n",
      "Got 890 / 1000 correct (89.00)\n",
      "Epoch accuracy time: 7.090\n",
      "Epoch 99 / 240\n",
      "t = 100, loss = 0.4081\n",
      "Epoch training time: 13.382\n",
      "Got 46095 / 49000 correct (94.07)\n",
      "Got 891 / 1000 correct (89.10)\n",
      "Epoch accuracy time: 7.161\n",
      "Epoch 100 / 240\n",
      "t = 100, loss = 0.3325\n",
      "Epoch training time: 13.397\n",
      "Got 46140 / 49000 correct (94.16)\n",
      "Got 891 / 1000 correct (89.10)\n",
      "Epoch accuracy time: 7.140\n",
      "Epoch 101 / 240\n",
      "t = 100, loss = 0.3630\n",
      "Epoch training time: 13.403\n",
      "Got 46407 / 49000 correct (94.71)\n",
      "Got 897 / 1000 correct (89.70)\n",
      "Epoch accuracy time: 7.202\n",
      "Epoch 102 / 240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 0.4069\n",
      "Epoch training time: 13.395\n",
      "Got 46305 / 49000 correct (94.50)\n",
      "Got 896 / 1000 correct (89.60)\n",
      "Epoch accuracy time: 7.102\n",
      "Epoch 103 / 240\n",
      "t = 100, loss = 0.2782\n",
      "Epoch training time: 13.393\n",
      "Got 46312 / 49000 correct (94.51)\n",
      "Got 893 / 1000 correct (89.30)\n",
      "Epoch accuracy time: 7.147\n",
      "Epoch 104 / 240\n",
      "t = 100, loss = 0.3365\n",
      "Epoch training time: 13.415\n",
      "Got 46307 / 49000 correct (94.50)\n",
      "Got 898 / 1000 correct (89.80)\n",
      "Epoch accuracy time: 7.159\n",
      "Epoch 105 / 240\n",
      "t = 100, loss = 0.3000\n",
      "Epoch training time: 13.389\n",
      "Got 46506 / 49000 correct (94.91)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.154\n",
      "Epoch 106 / 240\n",
      "t = 100, loss = 0.3131\n",
      "Epoch training time: 13.393\n",
      "Got 46417 / 49000 correct (94.73)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.179\n",
      "Epoch 107 / 240\n",
      "t = 100, loss = 0.3678\n",
      "Epoch training time: 13.398\n",
      "Got 46627 / 49000 correct (95.16)\n",
      "Got 896 / 1000 correct (89.60)\n",
      "Epoch accuracy time: 7.203\n",
      "Epoch 108 / 240\n",
      "t = 100, loss = 0.3907\n",
      "Epoch training time: 13.387\n",
      "Got 46696 / 49000 correct (95.30)\n",
      "Got 893 / 1000 correct (89.30)\n",
      "Epoch accuracy time: 7.202\n",
      "Epoch 109 / 240\n",
      "t = 100, loss = 0.3302\n",
      "Epoch training time: 13.415\n",
      "Got 46769 / 49000 correct (95.45)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.154\n",
      "Epoch 110 / 240\n",
      "t = 100, loss = 0.3944\n",
      "Epoch training time: 13.379\n",
      "Got 46702 / 49000 correct (95.31)\n",
      "Got 896 / 1000 correct (89.60)\n",
      "Epoch accuracy time: 7.146\n",
      "Epoch 111 / 240\n",
      "t = 100, loss = 0.3411\n",
      "Epoch training time: 13.375\n",
      "Got 46490 / 49000 correct (94.88)\n",
      "Got 899 / 1000 correct (89.90)\n",
      "Epoch accuracy time: 7.215\n",
      "Epoch 112 / 240\n",
      "t = 100, loss = 0.3435\n",
      "Epoch training time: 13.380\n",
      "Got 46751 / 49000 correct (95.41)\n",
      "Got 895 / 1000 correct (89.50)\n",
      "Epoch accuracy time: 7.224\n",
      "Epoch 113 / 240\n",
      "t = 100, loss = 0.3522\n",
      "Epoch training time: 13.418\n",
      "Got 46714 / 49000 correct (95.33)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.102\n",
      "Epoch 114 / 240\n",
      "t = 100, loss = 0.2947\n",
      "Epoch training time: 13.445\n",
      "Got 46572 / 49000 correct (95.04)\n",
      "Got 904 / 1000 correct (90.40)\n",
      "Epoch accuracy time: 7.140\n",
      "Epoch 115 / 240\n",
      "t = 100, loss = 0.2975\n",
      "Epoch training time: 13.369\n",
      "Got 46656 / 49000 correct (95.22)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.196\n",
      "Epoch 116 / 240\n",
      "t = 100, loss = 0.2801\n",
      "Epoch training time: 13.402\n",
      "Got 46464 / 49000 correct (94.82)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.129\n",
      "Epoch 117 / 240\n",
      "t = 100, loss = 0.2620\n",
      "Epoch training time: 13.412\n",
      "Got 46917 / 49000 correct (95.75)\n",
      "Got 899 / 1000 correct (89.90)\n",
      "Epoch accuracy time: 7.182\n",
      "Epoch 118 / 240\n",
      "t = 100, loss = 0.3000\n",
      "Epoch training time: 13.423\n",
      "Got 46830 / 49000 correct (95.57)\n",
      "Got 903 / 1000 correct (90.30)\n",
      "Epoch accuracy time: 7.191\n",
      "Epoch 119 / 240\n",
      "t = 100, loss = 0.2831\n",
      "Epoch training time: 13.389\n",
      "Got 46922 / 49000 correct (95.76)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.155\n",
      "Epoch 120 / 240\n",
      "t = 100, loss = 0.3231\n",
      "Epoch training time: 13.425\n",
      "Got 46891 / 49000 correct (95.70)\n",
      "Got 899 / 1000 correct (89.90)\n",
      "Epoch accuracy time: 7.141\n",
      "Epoch 121 / 240\n",
      "t = 100, loss = 0.3597\n",
      "Epoch training time: 13.380\n",
      "Got 46935 / 49000 correct (95.79)\n",
      "Got 895 / 1000 correct (89.50)\n",
      "Epoch accuracy time: 7.120\n",
      "Epoch 122 / 240\n",
      "t = 100, loss = 0.2877\n",
      "Epoch training time: 13.403\n",
      "Got 47012 / 49000 correct (95.94)\n",
      "Got 898 / 1000 correct (89.80)\n",
      "Epoch accuracy time: 7.110\n",
      "Epoch 123 / 240\n",
      "t = 100, loss = 0.2664\n",
      "Epoch training time: 13.407\n",
      "Got 47167 / 49000 correct (96.26)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.115\n",
      "Epoch 124 / 240\n",
      "t = 100, loss = 0.3034\n",
      "Epoch training time: 13.424\n",
      "Got 47027 / 49000 correct (95.97)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.139\n",
      "Epoch 125 / 240\n",
      "t = 100, loss = 0.3224\n",
      "Epoch training time: 13.390\n",
      "Got 46970 / 49000 correct (95.86)\n",
      "Got 896 / 1000 correct (89.60)\n",
      "Epoch accuracy time: 7.165\n",
      "Epoch 126 / 240\n",
      "t = 100, loss = 0.2631\n",
      "Epoch training time: 13.394\n",
      "Got 46897 / 49000 correct (95.71)\n",
      "Got 899 / 1000 correct (89.90)\n",
      "Epoch accuracy time: 7.161\n",
      "Epoch 127 / 240\n",
      "t = 100, loss = 0.2569\n",
      "Epoch training time: 13.381\n",
      "Got 47028 / 49000 correct (95.98)\n",
      "Got 897 / 1000 correct (89.70)\n",
      "Epoch accuracy time: 7.128\n",
      "Epoch 128 / 240\n",
      "t = 100, loss = 0.2255\n",
      "Epoch training time: 13.406\n",
      "Got 47156 / 49000 correct (96.24)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.171\n",
      "Epoch 129 / 240\n",
      "t = 100, loss = 0.3225\n",
      "Epoch training time: 13.422\n",
      "Got 46906 / 49000 correct (95.73)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.128\n",
      "Epoch 130 / 240\n",
      "t = 100, loss = 0.2997\n",
      "Epoch training time: 13.367\n",
      "Got 47257 / 49000 correct (96.44)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.188\n",
      "Epoch 131 / 240\n",
      "t = 100, loss = 0.2586\n",
      "Epoch training time: 13.391\n",
      "Got 46853 / 49000 correct (95.62)\n",
      "Got 891 / 1000 correct (89.10)\n",
      "Epoch accuracy time: 7.269\n",
      "Epoch 132 / 240\n",
      "t = 100, loss = 0.3120\n",
      "Epoch training time: 13.419\n",
      "Got 47309 / 49000 correct (96.55)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.113\n",
      "Epoch 133 / 240\n",
      "t = 100, loss = 0.3075\n",
      "Epoch training time: 13.407\n",
      "Got 47346 / 49000 correct (96.62)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.159\n",
      "Epoch 134 / 240\n",
      "t = 100, loss = 0.2883\n",
      "Epoch training time: 13.382\n",
      "Got 47425 / 49000 correct (96.79)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.219\n",
      "Epoch 135 / 240\n",
      "t = 100, loss = 0.3393\n",
      "Epoch training time: 13.397\n",
      "Got 47386 / 49000 correct (96.71)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.148\n",
      "Epoch 136 / 240\n",
      "t = 100, loss = 0.2592\n",
      "Epoch training time: 13.374\n",
      "Got 47367 / 49000 correct (96.67)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.142\n",
      "Epoch 137 / 240\n",
      "t = 100, loss = 0.3075\n",
      "Epoch training time: 13.383\n",
      "Got 47435 / 49000 correct (96.81)\n",
      "Got 897 / 1000 correct (89.70)\n",
      "Epoch accuracy time: 7.101\n",
      "Epoch 138 / 240\n",
      "t = 100, loss = 0.2573\n",
      "Epoch training time: 13.407\n",
      "Got 47416 / 49000 correct (96.77)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.209\n",
      "Epoch 139 / 240\n",
      "t = 100, loss = 0.3121\n",
      "Epoch training time: 13.398\n",
      "Got 47521 / 49000 correct (96.98)\n",
      "Got 894 / 1000 correct (89.40)\n",
      "Epoch accuracy time: 7.180\n",
      "Epoch 140 / 240\n",
      "t = 100, loss = 0.2830\n",
      "Epoch training time: 13.411\n",
      "Got 47473 / 49000 correct (96.88)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.101\n",
      "Epoch 141 / 240\n",
      "t = 100, loss = 0.2231\n",
      "Epoch training time: 13.421\n",
      "Got 47216 / 49000 correct (96.36)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.106\n",
      "Epoch 142 / 240\n",
      "t = 100, loss = 0.2660\n",
      "Epoch training time: 13.391\n",
      "Got 47386 / 49000 correct (96.71)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.123\n",
      "Epoch 143 / 240\n",
      "t = 100, loss = 0.2790\n",
      "Epoch training time: 13.408\n",
      "Got 47484 / 49000 correct (96.91)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.692\n",
      "Epoch 144 / 240\n",
      "t = 100, loss = 0.2609\n",
      "Epoch training time: 13.409\n",
      "Got 47156 / 49000 correct (96.24)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.461\n",
      "Epoch 145 / 240\n",
      "t = 100, loss = 0.2366\n",
      "Epoch training time: 13.419\n",
      "Got 47327 / 49000 correct (96.59)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.203\n",
      "Epoch 146 / 240\n",
      "t = 100, loss = 0.2121\n",
      "Epoch training time: 13.417\n",
      "Got 47558 / 49000 correct (97.06)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.835\n",
      "Epoch 147 / 240\n",
      "t = 100, loss = 0.2829\n",
      "Epoch training time: 13.391\n",
      "Got 47488 / 49000 correct (96.91)\n",
      "Got 909 / 1000 correct (90.90)\n",
      "Epoch accuracy time: 7.273\n",
      "Epoch 148 / 240\n",
      "t = 100, loss = 0.2719\n",
      "Epoch training time: 13.377\n",
      "Got 47561 / 49000 correct (97.06)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.121\n",
      "Epoch 149 / 240\n",
      "t = 100, loss = 0.2560\n",
      "Epoch training time: 13.416\n",
      "Got 47509 / 49000 correct (96.96)\n",
      "Got 909 / 1000 correct (90.90)\n",
      "Epoch accuracy time: 7.111\n",
      "Epoch 150 / 240\n",
      "t = 100, loss = 0.2724\n",
      "Epoch training time: 13.402\n",
      "Got 47587 / 49000 correct (97.12)\n",
      "Got 907 / 1000 correct (90.70)\n",
      "Epoch accuracy time: 7.164\n",
      "Epoch 151 / 240\n",
      "t = 100, loss = 0.2911\n",
      "Epoch training time: 13.417\n",
      "Got 47461 / 49000 correct (96.86)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.150\n",
      "Epoch 152 / 240\n",
      "t = 100, loss = 0.2629\n",
      "Epoch training time: 13.393\n",
      "Got 47680 / 49000 correct (97.31)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.121\n",
      "Epoch 153 / 240\n",
      "t = 100, loss = 0.2123\n",
      "Epoch training time: 13.388\n",
      "Got 47676 / 49000 correct (97.30)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.184\n",
      "Epoch 154 / 240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 0.3282\n",
      "Epoch training time: 13.405\n",
      "Got 47349 / 49000 correct (96.63)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.110\n",
      "Epoch 155 / 240\n",
      "t = 100, loss = 0.2459\n",
      "Epoch training time: 13.368\n",
      "Got 47730 / 49000 correct (97.41)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.135\n",
      "Epoch 156 / 240\n",
      "t = 100, loss = 0.2296\n",
      "Epoch training time: 13.386\n",
      "Got 47791 / 49000 correct (97.53)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.117\n",
      "Epoch 157 / 240\n",
      "t = 100, loss = 0.2898\n",
      "Epoch training time: 13.374\n",
      "Got 47780 / 49000 correct (97.51)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.087\n",
      "Epoch 158 / 240\n",
      "t = 100, loss = 0.2970\n",
      "Epoch training time: 13.412\n",
      "Got 47793 / 49000 correct (97.54)\n",
      "Got 907 / 1000 correct (90.70)\n",
      "Epoch accuracy time: 7.192\n",
      "Epoch 159 / 240\n",
      "t = 100, loss = 0.3327\n",
      "Epoch training time: 13.376\n",
      "Got 47791 / 49000 correct (97.53)\n",
      "Got 901 / 1000 correct (90.10)\n",
      "Epoch accuracy time: 7.167\n",
      "Epoch 160 / 240\n",
      "t = 100, loss = 0.2073\n",
      "Epoch training time: 13.348\n",
      "Got 47612 / 49000 correct (97.17)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.130\n",
      "Epoch 161 / 240\n",
      "t = 100, loss = 0.2258\n",
      "Epoch training time: 13.405\n",
      "Got 47740 / 49000 correct (97.43)\n",
      "Got 900 / 1000 correct (90.00)\n",
      "Epoch accuracy time: 7.131\n",
      "Epoch 162 / 240\n",
      "t = 100, loss = 0.2637\n",
      "Epoch training time: 13.381\n",
      "Got 47641 / 49000 correct (97.23)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.218\n",
      "Epoch 163 / 240\n",
      "t = 100, loss = 0.2301\n",
      "Epoch training time: 13.388\n",
      "Got 47714 / 49000 correct (97.38)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.154\n",
      "Epoch 164 / 240\n",
      "t = 100, loss = 0.2305\n",
      "Epoch training time: 13.382\n",
      "Got 47898 / 49000 correct (97.75)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.179\n",
      "Epoch 165 / 240\n",
      "t = 100, loss = 0.2514\n",
      "Epoch training time: 13.393\n",
      "Got 47901 / 49000 correct (97.76)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.087\n",
      "Epoch 166 / 240\n",
      "t = 100, loss = 0.2525\n",
      "Epoch training time: 13.375\n",
      "Got 47711 / 49000 correct (97.37)\n",
      "Got 904 / 1000 correct (90.40)\n",
      "Epoch accuracy time: 7.137\n",
      "Epoch 167 / 240\n",
      "t = 100, loss = 0.2605\n",
      "Epoch training time: 13.385\n",
      "Got 47604 / 49000 correct (97.15)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.090\n",
      "Epoch 168 / 240\n",
      "t = 100, loss = 0.2190\n",
      "Epoch training time: 13.385\n",
      "Got 47787 / 49000 correct (97.52)\n",
      "Got 909 / 1000 correct (90.90)\n",
      "Epoch accuracy time: 7.133\n",
      "Epoch 169 / 240\n",
      "t = 100, loss = 0.2705\n",
      "Epoch training time: 13.439\n",
      "Got 47943 / 49000 correct (97.84)\n",
      "Got 907 / 1000 correct (90.70)\n",
      "Epoch accuracy time: 7.159\n",
      "Epoch 170 / 240\n",
      "t = 100, loss = 0.2244\n",
      "Epoch training time: 13.420\n",
      "Got 47877 / 49000 correct (97.71)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.133\n",
      "Epoch 171 / 240\n",
      "t = 100, loss = 0.2150\n",
      "Epoch training time: 13.399\n",
      "Got 47756 / 49000 correct (97.46)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.151\n",
      "Epoch 172 / 240\n",
      "t = 100, loss = 0.2699\n",
      "Epoch training time: 13.409\n",
      "Got 47731 / 49000 correct (97.41)\n",
      "Got 905 / 1000 correct (90.50)\n",
      "Epoch accuracy time: 7.161\n",
      "Epoch 173 / 240\n",
      "t = 100, loss = 0.2313\n",
      "Epoch training time: 13.385\n",
      "Got 48068 / 49000 correct (98.10)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.086\n",
      "Epoch 174 / 240\n",
      "t = 100, loss = 0.3209\n",
      "Epoch training time: 13.399\n",
      "Got 48021 / 49000 correct (98.00)\n",
      "Got 912 / 1000 correct (91.20)\n",
      "Epoch accuracy time: 7.101\n",
      "Epoch 175 / 240\n",
      "t = 100, loss = 0.1921\n",
      "Epoch training time: 13.386\n",
      "Got 47747 / 49000 correct (97.44)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.117\n",
      "Epoch 176 / 240\n",
      "t = 100, loss = 0.2744\n",
      "Epoch training time: 13.397\n",
      "Got 47912 / 49000 correct (97.78)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.212\n",
      "Epoch 177 / 240\n",
      "t = 100, loss = 0.3257\n",
      "Epoch training time: 13.394\n",
      "Got 48038 / 49000 correct (98.04)\n",
      "Got 903 / 1000 correct (90.30)\n",
      "Epoch accuracy time: 7.098\n",
      "Epoch 178 / 240\n",
      "t = 100, loss = 0.2605\n",
      "Epoch training time: 13.376\n",
      "Got 48096 / 49000 correct (98.16)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.112\n",
      "Epoch 179 / 240\n",
      "t = 100, loss = 0.2228\n",
      "Epoch training time: 13.396\n",
      "Got 47770 / 49000 correct (97.49)\n",
      "Got 915 / 1000 correct (91.50)\n",
      "Epoch accuracy time: 7.167\n",
      "Epoch 180 / 240\n",
      "t = 100, loss = 0.2065\n",
      "Epoch training time: 13.381\n",
      "Got 47990 / 49000 correct (97.94)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.182\n",
      "Epoch 181 / 240\n",
      "t = 100, loss = 0.2334\n",
      "Epoch training time: 13.395\n",
      "Got 48098 / 49000 correct (98.16)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.108\n",
      "Epoch 182 / 240\n",
      "t = 100, loss = 0.1376\n",
      "Epoch training time: 13.385\n",
      "Got 48039 / 49000 correct (98.04)\n",
      "Got 912 / 1000 correct (91.20)\n",
      "Epoch accuracy time: 7.132\n",
      "Epoch 183 / 240\n",
      "t = 100, loss = 0.2192\n",
      "Epoch training time: 13.397\n",
      "Got 47861 / 49000 correct (97.68)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.131\n",
      "Epoch 184 / 240\n",
      "t = 100, loss = 0.1820\n",
      "Epoch training time: 13.379\n",
      "Got 47962 / 49000 correct (97.88)\n",
      "Got 913 / 1000 correct (91.30)\n",
      "Epoch accuracy time: 7.199\n",
      "Epoch 185 / 240\n",
      "t = 100, loss = 0.2393\n",
      "Epoch training time: 13.371\n",
      "Got 48057 / 49000 correct (98.08)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.214\n",
      "Epoch 186 / 240\n",
      "t = 100, loss = 0.2875\n",
      "Epoch training time: 13.376\n",
      "Got 48155 / 49000 correct (98.28)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.141\n",
      "Epoch 187 / 240\n",
      "t = 100, loss = 0.2518\n",
      "Epoch training time: 13.384\n",
      "Got 47960 / 49000 correct (97.88)\n",
      "Got 902 / 1000 correct (90.20)\n",
      "Epoch accuracy time: 7.175\n",
      "Epoch 188 / 240\n",
      "t = 100, loss = 0.1984\n",
      "Epoch training time: 13.380\n",
      "Got 48166 / 49000 correct (98.30)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.087\n",
      "Epoch 189 / 240\n",
      "t = 100, loss = 0.2046\n",
      "Epoch training time: 13.380\n",
      "Got 47964 / 49000 correct (97.89)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.103\n",
      "Epoch 190 / 240\n",
      "t = 100, loss = 0.2456\n",
      "Epoch training time: 13.350\n",
      "Got 48142 / 49000 correct (98.25)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.211\n",
      "Epoch 191 / 240\n",
      "t = 100, loss = 0.1988\n",
      "Epoch training time: 13.404\n",
      "Got 47978 / 49000 correct (97.91)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.189\n",
      "Epoch 192 / 240\n",
      "t = 100, loss = 0.2183\n",
      "Epoch training time: 13.374\n",
      "Got 48046 / 49000 correct (98.05)\n",
      "Got 912 / 1000 correct (91.20)\n",
      "Epoch accuracy time: 7.159\n",
      "Epoch 193 / 240\n",
      "t = 100, loss = 0.1933\n",
      "Epoch training time: 13.362\n",
      "Got 48055 / 49000 correct (98.07)\n",
      "Got 913 / 1000 correct (91.30)\n",
      "Epoch accuracy time: 7.150\n",
      "Epoch 194 / 240\n",
      "t = 100, loss = 0.2087\n",
      "Epoch training time: 13.384\n",
      "Got 48194 / 49000 correct (98.36)\n",
      "Got 906 / 1000 correct (90.60)\n",
      "Epoch accuracy time: 7.221\n",
      "Epoch 195 / 240\n",
      "t = 100, loss = 0.2452\n",
      "Epoch training time: 13.391\n",
      "Got 48166 / 49000 correct (98.30)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.216\n",
      "Epoch 196 / 240\n",
      "t = 100, loss = 0.1754\n",
      "Epoch training time: 13.393\n",
      "Got 48221 / 49000 correct (98.41)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.128\n",
      "Epoch 197 / 240\n",
      "t = 100, loss = 0.2452\n",
      "Epoch training time: 13.401\n",
      "Got 47947 / 49000 correct (97.85)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.164\n",
      "Epoch 198 / 240\n",
      "t = 100, loss = 0.1959\n",
      "Epoch training time: 13.373\n",
      "Got 48136 / 49000 correct (98.24)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.198\n",
      "Epoch 199 / 240\n",
      "t = 100, loss = 0.2011\n",
      "Epoch training time: 13.405\n",
      "Got 48006 / 49000 correct (97.97)\n",
      "Got 912 / 1000 correct (91.20)\n",
      "Epoch accuracy time: 7.190\n",
      "Epoch 200 / 240\n",
      "t = 100, loss = 0.2390\n",
      "Epoch training time: 13.403\n",
      "Got 48143 / 49000 correct (98.25)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.199\n",
      "Epoch 201 / 240\n",
      "t = 100, loss = 0.2088\n",
      "Epoch training time: 13.386\n",
      "Got 48231 / 49000 correct (98.43)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.189\n",
      "Epoch 202 / 240\n",
      "t = 100, loss = 0.2184\n",
      "Epoch training time: 13.343\n",
      "Got 48157 / 49000 correct (98.28)\n",
      "Got 909 / 1000 correct (90.90)\n",
      "Epoch accuracy time: 7.299\n",
      "Epoch 203 / 240\n",
      "t = 100, loss = 0.1910\n",
      "Epoch training time: 13.385\n",
      "Got 48267 / 49000 correct (98.50)\n",
      "Got 912 / 1000 correct (91.20)\n",
      "Epoch accuracy time: 7.226\n",
      "Epoch 204 / 240\n",
      "t = 100, loss = 0.1753\n",
      "Epoch training time: 13.403\n",
      "Got 48267 / 49000 correct (98.50)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.197\n",
      "Epoch 205 / 240\n",
      "t = 100, loss = 0.2873\n",
      "Epoch training time: 13.404\n",
      "Got 48250 / 49000 correct (98.47)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 912 / 1000 correct (91.20)\n",
      "Epoch accuracy time: 7.189\n",
      "Epoch 206 / 240\n",
      "t = 100, loss = 0.1706\n",
      "Epoch training time: 13.398\n",
      "Got 48318 / 49000 correct (98.61)\n",
      "Got 909 / 1000 correct (90.90)\n",
      "Epoch accuracy time: 7.065\n",
      "Epoch 207 / 240\n",
      "t = 100, loss = 0.1978\n",
      "Epoch training time: 13.389\n",
      "Got 48289 / 49000 correct (98.55)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.151\n",
      "Epoch 208 / 240\n",
      "t = 100, loss = 0.2467\n",
      "Epoch training time: 13.407\n",
      "Got 48066 / 49000 correct (98.09)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.166\n",
      "Epoch 209 / 240\n",
      "t = 100, loss = 0.2232\n",
      "Epoch training time: 13.412\n",
      "Got 48348 / 49000 correct (98.67)\n",
      "Got 913 / 1000 correct (91.30)\n",
      "Epoch accuracy time: 7.070\n",
      "Epoch 210 / 240\n",
      "t = 100, loss = 0.1758\n",
      "Epoch training time: 13.402\n",
      "Got 48314 / 49000 correct (98.60)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.124\n",
      "Epoch 211 / 240\n",
      "t = 100, loss = 0.1752\n",
      "Epoch training time: 13.397\n",
      "Got 48097 / 49000 correct (98.16)\n",
      "Got 916 / 1000 correct (91.60)\n",
      "Epoch accuracy time: 7.113\n",
      "Epoch 212 / 240\n",
      "t = 100, loss = 0.2083\n",
      "Epoch training time: 13.376\n",
      "Got 48082 / 49000 correct (98.13)\n",
      "Got 918 / 1000 correct (91.80)\n",
      "Epoch accuracy time: 7.139\n",
      "Epoch 213 / 240\n",
      "t = 100, loss = 0.2221\n",
      "Epoch training time: 13.364\n",
      "Got 48046 / 49000 correct (98.05)\n",
      "Got 920 / 1000 correct (92.00)\n",
      "Epoch accuracy time: 7.142\n",
      "Epoch 214 / 240\n",
      "t = 100, loss = 0.1632\n",
      "Epoch training time: 13.403\n",
      "Got 48112 / 49000 correct (98.19)\n",
      "Got 916 / 1000 correct (91.60)\n",
      "Epoch accuracy time: 7.175\n",
      "Epoch 215 / 240\n",
      "t = 100, loss = 0.1455\n",
      "Epoch training time: 13.403\n",
      "Got 48170 / 49000 correct (98.31)\n",
      "Got 913 / 1000 correct (91.30)\n",
      "Epoch accuracy time: 7.123\n",
      "Epoch 216 / 240\n",
      "t = 100, loss = 0.2062\n",
      "Epoch training time: 13.376\n",
      "Got 48325 / 49000 correct (98.62)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.118\n",
      "Epoch 217 / 240\n",
      "t = 100, loss = 0.1733\n",
      "Epoch training time: 13.387\n",
      "Got 48414 / 49000 correct (98.80)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.116\n",
      "Epoch 218 / 240\n",
      "t = 100, loss = 0.2011\n",
      "Epoch training time: 13.401\n",
      "Got 48172 / 49000 correct (98.31)\n",
      "Got 920 / 1000 correct (92.00)\n",
      "Epoch accuracy time: 7.181\n",
      "Epoch 219 / 240\n",
      "t = 100, loss = 0.1812\n",
      "Epoch training time: 13.380\n",
      "Got 48146 / 49000 correct (98.26)\n",
      "Got 912 / 1000 correct (91.20)\n",
      "Epoch accuracy time: 7.130\n",
      "Epoch 220 / 240\n",
      "t = 100, loss = 0.1930\n",
      "Epoch training time: 13.373\n",
      "Got 48360 / 49000 correct (98.69)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.137\n",
      "Epoch 221 / 240\n",
      "t = 100, loss = 0.2508\n",
      "Epoch training time: 13.384\n",
      "Got 48354 / 49000 correct (98.68)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.093\n",
      "Epoch 222 / 240\n",
      "t = 100, loss = 0.1679\n",
      "Epoch training time: 13.362\n",
      "Got 48387 / 49000 correct (98.75)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.170\n",
      "Epoch 223 / 240\n",
      "t = 100, loss = 0.1466\n",
      "Epoch training time: 13.383\n",
      "Got 48389 / 49000 correct (98.75)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.136\n",
      "Epoch 224 / 240\n",
      "t = 100, loss = 0.1854\n",
      "Epoch training time: 13.371\n",
      "Got 48179 / 49000 correct (98.32)\n",
      "Got 915 / 1000 correct (91.50)\n",
      "Epoch accuracy time: 7.144\n",
      "Epoch 225 / 240\n",
      "t = 100, loss = 0.1563\n",
      "Epoch training time: 13.392\n",
      "Got 48217 / 49000 correct (98.40)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.115\n",
      "Epoch 226 / 240\n",
      "t = 100, loss = 0.1845\n",
      "Epoch training time: 13.365\n",
      "Got 48352 / 49000 correct (98.68)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.137\n",
      "Epoch 227 / 240\n",
      "t = 100, loss = 0.1478\n",
      "Epoch training time: 13.389\n",
      "Got 48408 / 49000 correct (98.79)\n",
      "Got 921 / 1000 correct (92.10)\n",
      "Epoch accuracy time: 7.106\n",
      "Epoch 228 / 240\n",
      "t = 100, loss = 0.1723\n",
      "Epoch training time: 13.372\n",
      "Got 48302 / 49000 correct (98.58)\n",
      "Got 917 / 1000 correct (91.70)\n",
      "Epoch accuracy time: 7.170\n",
      "Epoch 229 / 240\n",
      "t = 100, loss = 0.1214\n",
      "Epoch training time: 13.404\n",
      "Got 48173 / 49000 correct (98.31)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.193\n",
      "Epoch 230 / 240\n",
      "t = 100, loss = 0.1835\n",
      "Epoch training time: 13.394\n",
      "Got 48418 / 49000 correct (98.81)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.120\n",
      "Epoch 231 / 240\n",
      "t = 100, loss = 0.1707\n",
      "Epoch training time: 13.392\n",
      "Got 48351 / 49000 correct (98.68)\n",
      "Got 911 / 1000 correct (91.10)\n",
      "Epoch accuracy time: 7.157\n",
      "Epoch 232 / 240\n",
      "t = 100, loss = 0.1989\n",
      "Epoch training time: 13.373\n",
      "Got 48121 / 49000 correct (98.21)\n",
      "Got 915 / 1000 correct (91.50)\n",
      "Epoch accuracy time: 7.204\n",
      "Epoch 233 / 240\n",
      "t = 100, loss = 0.1906\n",
      "Epoch training time: 13.372\n",
      "Got 48429 / 49000 correct (98.83)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.100\n",
      "Epoch 234 / 240\n",
      "t = 100, loss = 0.1582\n",
      "Epoch training time: 13.363\n",
      "Got 48214 / 49000 correct (98.40)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.200\n",
      "Epoch 235 / 240\n",
      "t = 100, loss = 0.1640\n",
      "Epoch training time: 13.393\n",
      "Got 48432 / 49000 correct (98.84)\n",
      "Got 909 / 1000 correct (90.90)\n",
      "Epoch accuracy time: 7.098\n",
      "Epoch 236 / 240\n",
      "t = 100, loss = 0.2041\n",
      "Epoch training time: 13.407\n",
      "Got 48424 / 49000 correct (98.82)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.109\n",
      "Epoch 237 / 240\n",
      "t = 100, loss = 0.1606\n",
      "Epoch training time: 13.401\n",
      "Got 48434 / 49000 correct (98.84)\n",
      "Got 914 / 1000 correct (91.40)\n",
      "Epoch accuracy time: 7.100\n",
      "Epoch 238 / 240\n",
      "t = 100, loss = 0.1852\n",
      "Epoch training time: 13.399\n",
      "Got 48437 / 49000 correct (98.85)\n",
      "Got 908 / 1000 correct (90.80)\n",
      "Epoch accuracy time: 7.118\n",
      "Epoch 239 / 240\n",
      "t = 100, loss = 0.1752\n",
      "Epoch training time: 13.403\n",
      "Got 48464 / 49000 correct (98.91)\n",
      "Got 913 / 1000 correct (91.30)\n",
      "Epoch accuracy time: 7.099\n",
      "Epoch 240 / 240\n",
      "t = 100, loss = 0.1574\n",
      "Epoch training time: 13.379\n",
      "Got 48449 / 49000 correct (98.88)\n",
      "Got 910 / 1000 correct (91.00)\n",
      "Epoch accuracy time: 7.161\n",
      "Total time: 4931.950\n",
      "Params: {'learning_rate': 0.0002, 'l2': 0.0006, 'dropout': 0.22, 'num_epochs': 240, 'optimizer_class': <class 'torch.optim.adam.Adam'>, 'model_builder': <function conv_conv_pool_2_dr_bn at 0x7fd598f58ae8>, 'loss_fn': CrossEntropyLoss(\n",
      "), 'transforms_fn': <function transforms_6 at 0x7fd598f399d8>, 'lr_scheduler': <function lr_scheduler_step_4 at 0x7fd598f397b8>, 'batch_size': 256}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from cs231n import hparam_search, plot_utils, models\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "#grid_params = hparam_search.grid_layout({\n",
    "#    \"learning_rate\": [0.00015331376711745498, 2e-4, 0.00027762985191595076, 4e-4, 0.0005441844914295301, 6e-4, 7e-4],\n",
    "#    \"l2\": [0, 3.2862408041294504e-05, 6.670482581523959e-05, 9.703458981971375e-05, 1e-4],\n",
    "#    \"dropout\": [0.3, 0.3842746183569362, 0.43443025833580695, 0.5013790481298508, 0.6701267471058839],\n",
    "#    \"optimizer_class\": [optim.RMSprop],\n",
    "#    \"model_builder\": [models.conv_relu_pool_4_dr_2],\n",
    "#    \"loss_fn\": [nn.CrossEntropyLoss().type(gpu_dtype)]\n",
    "#})\n",
    "\n",
    "grid_params = hparam_search.grid_layout({\n",
    "    \"learning_rate\": [2e-04],\n",
    "    \"l2\": [0.0006],\n",
    "    \"dropout\": [0.22],\n",
    "    \"num_epochs\": [240],\n",
    "    \"optimizer_class\": [optim.Adam],\n",
    "    \"model_builder\": [models.conv_conv_pool_2_dr_bn],\n",
    "    \"loss_fn\": [nn.CrossEntropyLoss().type(gpu_dtype)],\n",
    "    \"transforms_fn\": [models.transforms_6],\n",
    "    \"lr_scheduler\": [models.lr_scheduler_step_4],\n",
    "    \"batch_size\": [256],\n",
    "})\n",
    "\n",
    "random_params = hparam_search.random_layout({\n",
    "    \"learning_rate\": lambda: 10 ** np.random.uniform(-6, 1),\n",
    "    \"l2\": lambda: 10 ** np.random.uniform(-5, 0),\n",
    "    \"dropout\": lambda: np.random.uniform(0, 1)\n",
    "}, n=50)\n",
    "\n",
    "mix_params = hparam_search.grid_layout({\n",
    "    \"optimizer_class\": [optim.RMSprop],\n",
    "    \"model_builder\": [models.conv_conv_pool_2_dr_bn],\n",
    "    \"loss_fn\": [nn.CrossEntropyLoss().type(gpu_dtype)],\n",
    "    \"num_epochs\": [25],\n",
    "    \"params\": random_params\n",
    "})\n",
    "\n",
    "best_model = None\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for params in grid_params:\n",
    "#for params in random_params:\n",
    "#for params in mix_params:\n",
    "\n",
    "    transforms_fn = params[\"transforms_fn\"]\n",
    "    cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True, transform=transforms_fn())\n",
    "    loader_train = DataLoader(cifar10_train, batch_size=params[\"batch_size\"], sampler=ChunkSampler(NUM_TRAIN, 0), num_workers=4)\n",
    "    loader_val = DataLoader(cifar10_val, batch_size=params[\"batch_size\"], sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "\n",
    "    optimizer_class = params[\"optimizer_class\"]\n",
    "    model_builder = params[\"model_builder\"]\n",
    "    loss_fn = params[\"loss_fn\"]\n",
    "    #params = params[\"params\"]\n",
    "\n",
    "    model = model_builder(dropout=params[\"dropout\"])\n",
    "    model_name = str(model_builder.__name__)\n",
    "    model = model.type(gpu_dtype)\n",
    "    #model.apply(reset)\n",
    "    \n",
    "    optimizer = optimizer_class(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"l2\"])\n",
    "    lr_scheduler = params[\"lr_scheduler\"](optimizer)\n",
    "    \n",
    "    print(\"Params: %s\" % params)\n",
    "    train_result = train(model, loss_fn, optimizer, lr_scheduler=lr_scheduler,\n",
    "                         num_epochs=params[\"num_epochs\"], trace_acc=True)\n",
    "    print(\"Params: %s\" % params)\n",
    "    \n",
    "    t = datetime.utcnow().isoformat()\n",
    "    val_acc = train_result[\"val_acc\"][-1] * 100\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = model\n",
    "    \n",
    "    plot_path = \"training-results/%s_%.2f_%s.png\" % (model_name, val_acc, t)\n",
    "    plot_utils.plot_learning_curves(save_path=plot_path, **train_result)\n",
    "    \n",
    "    params_str = {}\n",
    "    for key, val in params.items():\n",
    "        params_str[key] = str(val)\n",
    "    result = {\n",
    "        \"model_name\": model_name,\n",
    "        \"params\": params_str,\n",
    "        \"train_result\": train_result,\n",
    "    }\n",
    "    result_path = \"training-results/%s_%.2f_%s.json\" % (model_name, val_acc, t)\n",
    "    with open(result_path, 'w') as fp:\n",
    "        json.dump(result, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially trained various models using random layout for hyperparameters search, then picked best models and narrowed parameters search by using fine grained grid layout close to parameters found using random layout. Learning rate decay was used to improve optimization and data augmentation to reduce overfitting. Below are listed things which helped with the process:\n",
    "- Added hparam_seach.py util to simplify generation of hyperparameters on grid and random layout.\n",
    "- Added plot_utils.py to display plots and save to file.\n",
    "- Extended train function to return training results, timing info and optionally validation set accuracy.\n",
    "- Implemented saving training results and hyperparameters to file.\n",
    "- Defined various models in models.py.\n",
    "- Increased batch size.\n",
    "- Learning rate decay (learning rate schedulers defined in models.py).\n",
    "- Extended train function to use learning rate scheduler.\n",
    "- Data augmentation (transformations defined in models.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model).  This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 8999 / 10000 correct (89.99)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8999"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(best_model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further with PyTorch\n",
    "\n",
    "The next assignment will make heavy use of PyTorch. You might also find it useful for your projects. \n",
    "\n",
    "Here's a nice tutorial by Justin Johnson that shows off some of PyTorch's features, like dynamic graphs and custom NN modules: http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "If you're interested in reinforcement learning for your final project, this is a good (more advanced) DQN tutorial in PyTorch: http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
